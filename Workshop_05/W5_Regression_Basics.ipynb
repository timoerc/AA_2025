{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 05` - Linear Regression\n",
    "\n",
    "This notebook introduced linear regression modeling in Python.\n",
    "\n",
    "We will go through the following:\n",
    "- manual implementation of gradient descent\n",
    "- linear regression using scikit-learn\n",
    "- multi-dimensional linear regression\n",
    "- regression evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for machine learning in Python\n",
    "\n",
    "Let's start with some information about the types of libraries we will use to run machine learning algorithms in Python. Although there are a number of machine learning packages available, by far the most popular Python library for general-purpose \"classical\" machine learning (this is in contrast to packages focused specficially on deep learning, such as [TensorFlow](http://www.tensorflow.org)) is the [scikit-learn](http://scikit-learn.org/) library. Scikit-learn is a general purpose machine learning library with a number of common machine learning algorithms built in.\n",
    "\n",
    "One important note, however, is that (despite some ongoing efforts to make it more scalable) scikit-learn is still best suited for small to medium-scale problems (say with ~10,000s of examples and ~1,000s of features). For these size problems, most of the algorithms contained in the library will work reasonably fast, and the library has the advantage that one can train many different types of algorithms all with the same interface. However, if you have data that is much bigger than this, then the algorithms start to get fairly slow compared to other more specialized libraries, and you are likely better off using an alternative library.\n",
    "\n",
    "Another important caveat - and this is one that sadly often gets ignored - is that unlike other software libraries, you _need_ to have some understanding of what the algorithms do in order to use scikit-learn effectively. This is because virtually all algorithms will have a substantial number of hyperparameters (= settings to the algorithm that can drastically affect performance). Sadly, a surprisingly large number of the statements people make about data science techniques seem less about the actual algorithms and more about whatever default settings scikit-learn happens to have for each algorithm. This is why you get people saying things like \"support vector machines worked better than neural networks for this problem\", which is a completely meaningless statement unless you know _what sort_ of support vector machine, and _what architecture_ neural network. Maybe in some years we will be at a place where the ML algorithms truly are \"self-contained\", and practitioners don't need to know anything about the underlying algorithms to get good performance (certainly, some researchers and companies are attempting to develop tools that move in this direction). But for the vast majority of tasks, we are still far away from this point, and you _do_ absolutely need to understand the algorithms to get reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting peak electrical power\n",
    "Suppose you want to predict what the peak electricity demand will be during the day tomorrow for some area (the data is from Pittsburgh). This is actually a very important problem from a logistics planning perspective: electricity generators, which for the most part are based upon boiling water to move turbines, cannot turn on instantly, so in order to guarantee that we have enough power to supply a given area, a system operator typically needs to have some excess generation always waiting in the wings. The better we can forecast future demand, the smaller our excess margin can be, leading to increased efficiency of the entire electrical grid.\n",
    "\n",
    "The power consumption tomorrow depends on many factors: temperature, day of week, season, holiday events, etc. - not to mention some inherent randomness that we don’t expect to even predict with perfect accuracy.\n",
    "\n",
    "What _is_ easy, however, is simply to collect lots of data about past energy consumption (the system operator serving the Pittsburgh region, PJM, maintains a data set available [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition)) as well as the past factors that affect consumption, like the past weather for the area. We combined this into the `Pittsburgh_load_data.csv` dataset (in the `data` folder in the project root directory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's import the dataset. Note that 'AVG', 'MAX', and 'MIN' are in GW, 'Total' in GWh, and 'High_temp' and 'Avg_temp' in degrees Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Pittsburgh_load_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'Date'-column\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"],format=\"%d.%m.%Y\")\n",
    "df = df.sort_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect raw data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot peak demand over time\n",
    "fig, ax = plt.subplots(figsize = (16,9))\n",
    "ax.plot(df[\"Date\"],df[[\"MAX\"]])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of this notebook, we will only consider electricity consumption during the summer months, so let's create a separate dataframe for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month feature\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda dt: dt.month)\n",
    "\n",
    "# create df_summer\n",
    "summer_month=[6,7,8]\n",
    "df_summer = df[df[\"Month\"].isin(summer_month)==True].copy()\n",
    "\n",
    "df_summer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the summer months, there seems to be a strong correlation between temperature and peak electricity demand, as becomes evident when looking at the scatter plot of these two features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(df_summer[\"High_temp\"], df_summer[\"MAX\"], marker='x',)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: intuition\n",
    "\n",
    "As it turns out, electricity consumption in the summer in Pittsburgh is largely\n",
    "driven by air conditioning, so with increasing high temperature comes increasing\n",
    "electricity demand.  Thus, we may hypothesize that we can form a fairly good prediction of the peak demand using a linear model: that is, we hypothesize that \n",
    "\\begin{equation}\n",
    "\\mathrm{PeakDemand} \\approx \\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "where $\\theta_1$ is the slope of the line and $\\theta_2$ is the intercept term (together called the _parameters_ of the model).  \n",
    "\n",
    "For example, just eyeballing the data we might guess that the slope for peak demand data and average demand data is approximately $\\theta_1 = 0.07$ (i.e., we get an increase of 0.07 GW per degree C, which we just arrived at by seeing that the power increased, very approximately, by noticing a total of ~1 GW increase in the range of 14 to 28 degree).  If we further suppose that the average peak demand is about 2.1 GW at 26 degrees C, then we can solve for the intercept term by $0.07*26 + \\theta_2 = 2.1$, or $\\theta_2 = 0.28$.  Note: if this seems ad-hoc to you, don't worry, it is! We will shortly decribe to actually find good values for these parameters in a disciplined way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "xp = df_summer['High_temp']\n",
    "yp = df_summer['MAX']\n",
    "\n",
    "# define theta vector with our estimated values for theta1 and theta2\n",
    "theta = np.array([0.07, 0.28])\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())  # gets us the limits of x and y\n",
    "ax.plot(xlim,[theta[0]*xlim[0]+theta[1], theta[0]*xlim[1]+theta[1]], 'C1')\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, this model won't fit the data exactly (we can see from the chart that the figure doesn't lie precisely on an exact line), but if we can find slope and intercept terms that fit the data well, then we can simply plug in tomorrow's forecasted high temperature into the equation above and get an estimate of the peak demand tomorrow (ignore the fact, for now, that the high temperature tomorrow is also a prediction - we'll assume we just get this from a reliable source, and domains like weather forecasting are extremely well-studied in practice). This is of course equivalent to just \"finding a point on the line\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual implementation of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: hypothesis, loss & data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question, of course, is how to find \"the best possible\" values for $\\theta_1$ and $\\theta_2$, so that the line fits the data as \"closely\" as possible. We will first use the method called _gradient descent_, which is an _extremely_ powerful and general algorithm. However, as you know from the lectures, there is an analytical solution to finding the best model parameters of a linear regression.\n",
    "\n",
    "**Objective functions**:  In order to find good values for the parameters, we need to formally define what \"good\" means in this setting.  This will actually be one of the key questions for machine learning algorithms in general, and different notions of \"goodness\" lead to different algorithms.  Fortunately, there are some very well-studied definitions in this context, and so we have some \"standard\" options that we can try.  The notion that we will consider here captures the idea of the \"squared error\" between the prediction and the actual values. That is, we consider all the days in the plot above, where $\\mathrm{HighTemperature}^{(i)}$ denotes the high temperature and $\\mathrm{PeakDemand}^{(i)}$ denotes the peak demand on day $i$. Since _predicted_ peak demand for day $i$ is equal to\n",
    "\\begin{equation}\n",
    "\\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "we want to make this quantity as close as possible, averaged over all the days, to the true $\\mathrm{PeakDemand}^{(i)}$.  We're going to measure this closeness in terms of the squared difference between the predicted and actual peak demand.  More formally, we would like to minimize the quantity:\n",
    "\\begin{equation}\n",
    "\\frac{1}{\\# \\mathrm{days}} \\sum_{i \\in \\mathrm{days}} \\left ( \\theta_1 \\cdot \\mathrm{HighTemperature}^{(i)} + \\theta_2 - \\mathrm{PeakDemand}^{(i)} \\right )^2 \\equiv E(\\theta)\n",
    "\\end{equation}\n",
    "which we abbreviate as $E(\\theta)$ to emphasize the fact that we are going to be minimizing this error by tuning our $\\theta$ variables.  This is known as the _objective function_ that we are trying to minimize.  A natural question that you may be asking is: why did we choose to measure closeness using this squared difference?  Why not use the average of absolute difference?  Or the maximum absolute difference?  These are good questions, and we'll defer answering them for now, except for saying that we will definitely consider other possibilities later.  The squared error is simply a very common choice, mainly for reasons of mathematical convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data scaling**:  We'll shortly see what the gradient descent procedure looks like in our example above.  Before we apply the algorithm, though, we're going to make one small modification to our problem, and _normalize_ the data (both the inputs and the output) before we attempt to run the gradient descent algorithm.  We will see the reason for this more clearly shortly, but the brief reason is that if our slope and intercept terms are on very different \"scales\" (which would be the case here, because of the relatively large input values (units of degrees Celsius) compared to the output values (units of gigawatts)), then we would actually need to take very different step sizes in the two parameters $\\theta_1$ and $\\theta_2$.  This is possible to manually tune in our case, but when we start having many more parameters, it's not feasible.  We thus make our life much easier if we scale all the input and output data to be in the same rough range _before_ running gradient descent (Note: in class we talked about only scaling the input, which also happens to work fine here, but I think it may be easier conceptually to consider the case where we just normalize all the inputs and outputs in the same manner).\n",
    "\n",
    "We can re-scale data in a number of ways, but a simple strategy is just to translate and scale the coordinates such that the values vary between zero and one in our dataset.  This can be easily achieved by the transformation\n",
    "\\begin{equation}\n",
    "\\tilde{x}^{(i)} = \\frac{x^{(i)} - \\min(x)}{\\max(x) - \\min(x)}\n",
    "\\end{equation}\n",
    "and similarly for $\\tilde{y}^{(i)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize xp and yp\n",
    "xp_nor = (xp-min(xp))/(max(xp)-min(xp))\n",
    "yp_nor = (yp-min(yp))/(max(yp)-min(yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the difference after re-scaling\n",
    "fig, axes = plt.subplots(1,2,figsize = (16,9))\n",
    "axes[0].scatter(xp, yp, marker='x')\n",
    "axes[0].set_xlabel(\"Temperature\")\n",
    "axes[0].set_ylabel(\"Demand\")\n",
    "axes[1].scatter(xp_nor, yp_nor, marker = 'x')\n",
    "axes[1].set_xlabel(\"Re-scaled Temperature\")\n",
    "axes[1].set_ylabel(\"Re-scaled Demand\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot looks identical to the right plot, of course, except that the units no longer correspond to traditional quantities like degrees Celsius or gigawatts, but just some linear transformation of these units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the gradient descent algorithm, which we have derived mathematically in the lectures.  This will initialize $\\theta_1$ and $\\theta_2$ to zero and repeatedly update them according to the partial derivative rules.  We will use the step size (also known as learning rate) $\\alpha=0.5$, and print out the value of $\\theta$ per each iteration.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j  := \\theta_j  − \\alpha \\sum_{j=1}^m (\\sum_{i=1}^n \\theta_j x_j^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Note: The above simplified equation omits the number of days (n) and the *2 term which results from partial differentiation and groups them into the alpha term for easier representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation of gradient descent\n",
    "def gradient_descent(iters, alpha):\n",
    "    theta = np.array([0., 0.])\n",
    "    for t in range(iters):\n",
    "        theta_old = theta.copy()\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta_old[0] * xp_nor + theta_old[1] - yp_nor)*xp_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta_old[0] * xp_nor + theta_old[1] - yp_nor))\n",
    "    return theta\n",
    "\n",
    "def plot_fit(theta):\n",
    "    \n",
    "    Error = sum((theta[0]*xp_nor + theta[1] - yp_nor)**2)\n",
    "    \n",
    "    # compute partial derivative (i.e. gradient) for theta 1 and 2\n",
    "    def_theta1 = sum((theta[0] * xp_nor + theta[1] - yp_nor)*xp_nor)\n",
    "    def_theta2 = sum((theta[0] * xp_nor + theta[1] - yp_nor))\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(xp_nor, yp_nor, marker = 'x')\n",
    "    plt.xlabel(\"Nomalized Temperature\")\n",
    "    plt.ylabel(\"Normalized Demand\")\n",
    "    xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "    plt.plot(xlim, [theta[0]*xlim[0]+theta[1], theta[0]*xlim[1]+theta[1]], 'C1')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    print('Theta = ', theta, 'Error = ',Error,'def_theta1 = ',def_theta1, 'def_theta2 = ', def_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "plot_fit(gradient_descent(0, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(1, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(2, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(3, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(4, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(5, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(100, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five iterations\n",
    "plot_fit(gradient_descent(1000, alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs. iteration\n",
    "\n",
    "Let's look at the average error versus iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_err(iters):\n",
    "    err = []\n",
    "    theta = np.array([0., 0.])\n",
    "    alpha = 0.5\n",
    "    for t in range(iters):\n",
    "        theta_old = theta.copy()\n",
    "        err.append(np.mean((theta_old[0] * xp_nor + theta_old[1] - yp_nor)**2))\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta_old[0] * xp_nor + theta_old[1] - yp_nor)*xp_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta_old[0] * xp_nor + theta_old[1] - yp_nor) )\n",
    "    return np.array(err)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(np.arange(0,50), gradient_descent_err(50))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the step size (learning rate)\n",
    "\n",
    "When using gradient descent, one of the hyperparameters we need to set is the step size (or learning rate) denoted by $\\alpha$ in the gradient descent updating formula.\n",
    "\n",
    "To understand the effect of different choices for $\\alpha$, let us visualize the gradient descent iterations from our example above in a different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create contours of loss function using grid of theta1 and theta2 values and generating the corresponding error for all combinations\n",
    "theta1_vals = np.linspace(0, 1.6, 100)\n",
    "theta2_vals = np.linspace(-0.3, 0.5, 100)\n",
    "se_vals = np.zeros(shape=(theta1_vals.size, theta2_vals.size)) # placeholder to be populated with errors below\n",
    "for i, val_2 in enumerate(theta2_vals):\n",
    "    for j, val_1 in enumerate(theta1_vals):\n",
    "        se_vals[i, j] = sum((val_1*xp_nor + val_2 - yp_nor)**2)\n",
    "\n",
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.5)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.5)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's illustrate what happens if we set $\\alpha$ to, for example, 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.6)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.6)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if this was a smart move. We now arrived at a lower loss after 50 iterations compared to $\\alpha = 0.5$, so let's increase our learning rate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.7)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.7)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks interesting. It appears as if we are still converging, but not necessarily faster compared to $\\alpha = 0.5$ or $\\alpha = 0.6$. What happens if we still increase the step size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.8)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.8)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $\\alpha = 0.8$, it looks like we are diverging - we are \"overshooting the target\" and actually moving further away from the lowest error with each iteration. As you can see, choosing an appropriate learning rate is crucial to balance the tradeoff between converging very slowly and not converging at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the solution back to the original scales\n",
    "\n",
    "Remember that we normalized our data before running gradient descent. Therefore, we still need to get our parameters back to the original scales. Fortunately, there is an easy way to do this, given by the following proof:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\tilde{y} & \\approx \\tilde{x} \\cdot \\theta_1 + \\theta_2\\\\\n",
    "\\Longrightarrow \\;\\; \\frac{y-a}{b} & \\approx \\frac{x-c}{d} \\cdot \\theta_1 + \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y-a & \\approx ((x-c)\\cdot \\theta_1)/d)\\cdot b + b \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot (b \\theta_1/d) + b \\theta_2 + a - c b \\theta_1/d\\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot \\hat{\\theta}_1 + \\hat{\\theta}_2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "a = \\min_i y^{(i)}, \\;\\; b = \\max_i y^{(i)} - \\min_i y^{(i)}, \\;\\; c = \\min_i x^{(i)}, \\;\\; d = \\max_i x^{(i)} - \\min_i x^{(i)}, \n",
    "\\end{equation}\n",
    "and where we define\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\theta}_1 & = b \\theta_1/d \\\\\n",
    "\\hat{\\theta}_2 & = b \\theta_2 + a - c \\cdot(b \\theta_1/d).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This might seem like a lot, but all it's saying is that there is an easy formula to convert between the solution we get for the normalized data and the unnormalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta 1 and theta 2 from gradient descent\n",
    "theta = gradient_descent(1000, alpha=0.5)\n",
    "\n",
    "# print thetas on normalized scale\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above expression, let's re-scale the theta parameters to the original scale by creating a new array entitled `theta_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a, b, c, d\n",
    "a = min(yp)\n",
    "b = max(yp)-min(yp)\n",
    "c = min(xp)\n",
    "d = max(xp) - min(xp)\n",
    "\n",
    "# use formula from above to re-scale\n",
    "theta_1 = b*theta[0]/d\n",
    "theta_2 = b*theta[1] + a - c*(b*theta[0]/d)\n",
    "\n",
    "#combine into single array\n",
    "theta_hat = np.array([theta_1,theta_2])\n",
    "\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the derived linear regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "ax.plot(xlim, [theta_hat[0]*xlim[0]+theta_hat[1], theta_hat[0]*xlim[1]+theta_hat[1]], 'C1')\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "print(theta, theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using scikit-learn\n",
    "\n",
    "Let's look at how we specify a model, fit it to data, and make predictions in scikit-learn. These three tasks form the common usage pattern for most interactions with scikit-learn. Let's first prepare our data. Note that scikit-learn by default will fit a separate intercept term for linear regression models, so we don't include the all-ones entry in our features.\n",
    "\n",
    "In general, scikit-learn has a different class for each different type of learning algorithm. In this case, we are importing the LinearRegression class. When we initialize the class, we pass various parameters to the constructor. In this case, we are specifying that we will fit an intercept term (i.e., we will not pass it as an explicit feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "linear_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "print(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we create this class we haven't actually passed any data to the system.  This is the standard interface for scikit-learn classes: the constructor just initializes the hyperparameters of the model, and when we actually want to fit it to data, we call the `model.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "linear_model.fit(xp.values.reshape((-1,1)), yp) # if we pass a 1-feature array we need to re-shape it! This is not required for multi-dimensional arrays\n",
    "print(linear_model.coef_, linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to own implementation (at 1000 iteration of gradient descent)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we want to make a prediction on a new data point, we call the model.predict() function, passing in the feature values for the new points we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new data\n",
    "Xnew = np.array([[25], [23]]) # predict peak load at 25 deg c\n",
    "\n",
    "print(linear_model.predict(Xnew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for original data\n",
    "model_pred = linear_model.predict(xp.values.reshape(-1,1))\n",
    "\n",
    "# plot regression line of our scikit-learn model\n",
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(xp, yp, marker=\"x\", label='Observed days')\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "ax.plot(xp, model_pred, c='C2', label='Squared loss fit')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional linear regression\n",
    "\n",
    "How do we now incorporate multiple features into our model? Essentially, all we need to do is to pass a larger feature vector to our model.\n",
    "\n",
    "To illustrate, let us consider differences between weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a weekday column to our data frame\n",
    "df_summer[\"Weekday\"] = df_summer[\"Date\"].dt.weekday\n",
    "\n",
    "# create IsWeekday feature\n",
    "df_summer[\"IsWeekday\"] = (df_summer[\"Weekday\"] <= 4).astype(int)\n",
    "\n",
    "# inspect dataframe\n",
    "df_summer.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "sns.scatterplot(x=df_summer[\"High_temp\"], y=df_summer[\"MAX\"], marker='o', hue=df_summer[\"IsWeekday\"], palette=\"mako\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train a new linear model with an additional feature for weekdays. As mentioned, all we need to do is to pass a larger feature vector to our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summer[[\"High_temp\", \"IsWeekday\"]]\n",
    "y = df_summer[[\"MAX\"]]\n",
    "lin_mod_day = LinearRegression()\n",
    "lin_mod_day.fit(X,y)\n",
    "y_pred = lin_mod_day.predict(X)\n",
    "\n",
    "print(lin_mod_day.coef_, lin_mod_day.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two feature vectors for weekdays and weekends\n",
    "X_weekday = X[X[\"IsWeekday\"]==1]\n",
    "X_weekend = X[X[\"IsWeekday\"]==0]\n",
    "\n",
    "# predict weekdays and weekends separately (using the same model!)\n",
    "y_pred_weekday = lin_mod_day.predict(X_weekday)\n",
    "y_pred_weekend = lin_mod_day.predict(X_weekend)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(X_weekday[\"High_temp\"], \n",
    "            df_summer[df_summer[\"IsWeekday\"]==1][\"MAX\"], \n",
    "            marker=\"+\", label=\"Observed Weekdays\")\n",
    "\n",
    "plt.scatter(X_weekend[\"High_temp\"], \n",
    "            df_summer[df_summer[\"IsWeekday\"]==0][\"MAX\"], \n",
    "            marker=\"x\", label=\"Observed Weekends\")\n",
    "\n",
    "plt.plot(X_weekday[\"High_temp\"], \n",
    "         y_pred_weekday, \n",
    "         label=\"Weekday Prediction\")\n",
    "\n",
    "plt.plot(X_weekend[\"High_temp\"], \n",
    "         y_pred_weekend, \n",
    "         label=\"Weekend Prediction\")\n",
    "\n",
    "plt.xlabel(\"High Temperature (°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation metrics\n",
    "\n",
    "We can easily quantify the distance between our predictions and the observed values using several evaluation metrics (see lecture slides). We will shortly summarize three commonly used metrics and show you how you can generate them. **However, keep in mind that we have not split our data into train/holdout/test sets, so the following metrics represent the training loss only. They do not say anything about predictive performance! We will implement a proper cross-validation procedure in the next workshop.**\n",
    "\n",
    "Let's use the true values and predictions of our multi-dimensional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Root) Mean Squared Error ((R)MSE)\n",
    "The MSE calculates the mean of the squared residuals. As the fitting of the linear regression model is (typically) carried out by minimizing the squared error (see loss function in lecture slides), the MSE corresponds directly to the model optimization. The unit of the MSE is the target unit squared, i.e. in the above example using energy consumption in GW, the unit of the MSE is $(GW)^2$. As the residuals (the difference of prediction and observed values) are squared, large deviations are penalized stronger than small deviations. Therefore, the weight of outliers increases using MSE as a metric. This is useful in applications where small prediction errors are not important but large errors have to be avoided.\n",
    "\n",
    "As the squared unit is hard to interpret, the RMSE can be used instead of the MSE. The RMSE is just the square root of the MSE, meaning that it is monotonic with respect to the MSE - a bigger MSE leads to a bigger RMSE and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\",mean_squared_error(df_summer[\"MAX\"], y_pred),\"(GW)^2\")\n",
    "print(\"Root Mean Squared Error:\",mean_squared_error(df_summer[\"MAX\"], y_pred)**0.5,\"GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Just as the RMSE, the MAE is easy to interpret. It is just the mean absolute value of the error term. When there are no or little major outliers, RMSE and MAE often have the same order of magnitude, with the MAE always being smaller than the RMSE. While a linear regression is typically fitted using the least squares method, the MAE can still be a valuable metric. It is applicable when large errors are not disproportionately worse than smaller errors. For example, when prediciting monetary values, an error of 100 USD might always be twice as bad as an error of 50 USD and so on. If this is the case, the MAE can actually be the more suitable error metric. Like the RSME, the MAE only makes sense when knowing the order of magnitude of the predicted values. On its own, it does not imply good or bad model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error:\",mean_absolute_error(df_summer[\"MAX\"], y_pred),\"GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Determination ($R^2$)\n",
    "The coefficient of determination (typically pronounced as \"R squared\"), is a metric of how well a model explains variance seen in the data. $R^2$ indicates the ratio of the explained and the overall variance (given some assumptions that we will not discuss here). Its value is always between 0 and 1. It can therefore be used as a means of comparison not only between regression methods, but also between completely different datasets - or even without knowing the data at all. For example, a value of 0.9 is always good, a value of 0.1 is always bad. Still, what threshold you would define as a good model fit depends on the domain of application. If you expect a high degree of randomness in your data, it is harder to explain your variance using a predictor. If you expect your data to be highly deterministic, then it should be easily explainable using suitable features and prediction methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient of determination:\",r2_score(df_summer[\"MAX\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
