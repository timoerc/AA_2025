{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 07` - Linear Classification\n",
    "\n",
    "In this workshop we will dive into classification. Classification is the prediction of classes. This means our target variable can take only a set of discrete values (versus the continuous case for regresssion). \n",
    "\n",
    "We will cover the following: \n",
    "1. Introduction to Linear Classification\n",
    "1. Linear Classification Using Support Vector Machines\n",
    "1. Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Regression to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas our previous introduction to machine learning focused on the case of predicting real-valued outputs (called the regression setting), these notes consider the case of predicting discrete-valued outputs. This is called the classification setting. And while it may seem as though this would require very different techniques to handle (after all, the regression setting we just considered made very \"continuous\" predictions rather than discrete ones), one nice thing about our generic framework for machine learning is that in reality, there is very little difference between how we handle regression and classification problems. Like before, there are hypothesis functions, loss functions, and our goal is still to solve the canonical problem of minimizing the sum of losses on a data set (the only real difference is in the choice of loss function, as we will see even the hypothesis class largely remains the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider a very simple illustrative example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True) # suppress scientific notation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create illustrative data\n",
    "x1 = np.linspace(0, 3, 20)\n",
    "x2 = np.linspace(5, 10, 20)\n",
    "y1 = np.zeros(20)\n",
    "y2 = np.ones(20)\n",
    "\n",
    "plt.scatter(x1, y1, c='red')\n",
    "plt.scatter(x2, y2, c='yellow')\n",
    "plt.xlabel(\"Feature A\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a dataset with only a single feature A and two classes, namely red and yellow. For larger values of feature A, it appears that observations are more likely to belong to class yellow. A linear classifier would attempt to draw a straight line separating the two classes. For this one-dimensional case, this is something we could do by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create illustrative data\n",
    "x1 = np.linspace(0, 3, 20)\n",
    "x2 = np.linspace(5, 10, 20)\n",
    "y1 = np.zeros(20)\n",
    "y2 = np.ones(20)\n",
    "\n",
    "plt.scatter(x1, y1, c='red')\n",
    "plt.scatter(x2, y2, c='yellow')\n",
    "plt.plot([4, 4], [-0.1, 1.1], color='grey')\n",
    "plt.plot([4.5, 4.5], [-0.1, 1.1], color='grey')\n",
    "plt.plot([3.7, 3.7], [-0.1, 1.1], color='grey')\n",
    "plt.plot([4.1], [0.5], 'x', color='green', markeredgewidth=2, markersize=10) # add a new datapoint illustrating a new observation\n",
    "plt.xlabel(\"Feature A\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might notice one problem: where exactly should we draw the line? This question is quite relevant, as you can see by considering a new observation (represented by the green X). Depending on the chosen line, we would classify this observation as either red or yellow. Also realize how the line we are searching for here is different from the line we were fitting in linear regression. **Instead of \"fitting a line to the data\", we are now \"separating the classes with a line\".**\n",
    "\n",
    "\n",
    "Let us now move on and pretend we have two features, namely A and B, instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.xlabel(\"Feature A\")\n",
    "plt.ylabel(\"Feature B\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that Features A and B provide information on class affiliation. A linear discriminative classifier would attempt to draw a straight line (or hyperplane for that matter) separating the two sets of data, and thereby create a model for classification. For two-dimensional data like that shown here, we could still do this by hand. The problem, however, remains: there is more than one possible separating boundary that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='green', markeredgewidth=2, markersize=10) # add a new datapoint illustrating a new observation\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "plt.xlabel(\"Feature A\")\n",
    "plt.ylabel(\"Feature B\")\n",
    "plt.xlim(-1, 3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label! Again, notice how \"drawing a line\" is different in this classification setting from fitting a linear regression line as we used to before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example: Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example to introduce classification problems, we're going to turn to a classical example from the machine learning literature, first presented in [[Street et al, 1993]](https://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/), namely identifying whether cancer cells from a biopsy are malignant or benign.  This was historically an important dataset in the intersection of machine learning and health care, and similar subjects are still a topic of very active research.  Of course, 25 years ago, the techniques were much more primitive and computationally less intensive than current approaches to similar problems, but the work has become a classical dataset in machine learning, and also represents a compelling applications.\n",
    "\n",
    "To create the data set, researchers took 569 images of cancerous cells, under a microscope, and manually selected the outlines of the different cells (this step is the kind of thing that would ideally be replaced by automatic computer vision architectures in current systems).  The resulting figures looked like this:\n",
    "\n",
    "![Image of (manually) selected outlines of cells under a microscope.](cancercells.png)\n",
    "\n",
    "The researchers then considered 10 different features of each cell, for instance the area, perimeter, texture, number of concave points (i.e., indentations), variance of grayscale color, and some others (a full list can be found in the above paper).  For each of these 10 features they computed the mean, max, and standard deviation of the feature across all the cells in the image, resulting in a 30 dimensional vector describing each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "print(\"Number of benign samples:\", len(cancer_df[cancer_df[\"diagnosis\"]==\"B\"]))\n",
    "print(\"Number of malignant samples:\", len(cancer_df[cancer_df[\"diagnosis\"]==\"M\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first plot a single feature, namely `area_mean`, and see how the results differ for **malignant** and **benign** tumors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function scales class_membership so that M equals 1 and B equals -1\n",
    "def rescale_classes(class_membership, true_label='M'):\n",
    "    return (class_membership == true_label).astype(int) * 2 - 1\n",
    "\n",
    "# this function plots the samples\n",
    "def plot_cells(variable='area_mean'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    malign = cancer_df[cancer_df[\"diagnosis\"]==\"M\"]\n",
    "    benign = cancer_df[cancer_df[\"diagnosis\"]==\"B\"]\n",
    "    plt.scatter(benign[variable], rescale_classes(benign['diagnosis']), marker='x', color='orange')\n",
    "    plt.scatter(malign[variable], rescale_classes(malign['diagnosis']), marker='x', color='red')\n",
    "    plt.plot([0, 2600], [0,0], color='lightgrey', linestyle='dotted')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([-1.1,1.1])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    \n",
    "plot_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the tumors based on their mean area, let us first stick with what we know: linear regression. With linear regression, we can fit a line that aims to predict -1 or 1 for each tumor using a squared error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "x = cancer_df['area_mean'].values.reshape(-1,1)\n",
    "y = rescale_classes(cancer_df['diagnosis'])\n",
    "\n",
    "# fit model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_reg = LinearRegression(fit_intercept=True)\n",
    "linear_reg.fit(x, y)\n",
    "\n",
    "# plot\n",
    "plot_cells()\n",
    "xfit = np.linspace(0, 2600)\n",
    "plt.plot([790, 790], [-1.1, 1.1], color='grey', linestyle='--')\n",
    "plt.plot(xfit, linear_reg.predict(xfit.reshape(-1,1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line plots the hypothesized values. Note how we can derive our separating boundary (the grey dashed line): it is located at the point for which our linear regression predicts precisely 0, i.e. the point where we would not know whether to predict if this is a malignant or benign tumor.\n",
    "\n",
    "**Question**: What do you think about our classification model? Is this the optimal separating boundary? Do you see any problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Loss Functions for Classification\n",
    "\n",
    "As we have seen above, our beloved mean squared error does not seem to be a great choice of a loss function in a classification setting. Luckily, we have other options at our disposal. There are many such losses, and different choices lead to different machine learning algorithms (often with very little indication that all that differs between the algorithm is in fact the choice of loss function).  Let's consider a few of the more common ones:\n",
    "\n",
    "- **0/1 loss:** \n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{0/1}}(h_\\theta(x), y) = \n",
    "      \\begin{cases}\n",
    "      0 & \\text{if sign $h_\\theta(x) = y$}\\\\\n",
    "      1 & \\text{otherwise}\n",
    "      \\end{cases}\n",
    "\\end{equation}\n",
    "Major disadvantage: function is non-smooth and therefore cannot be used with algorithms like gradient descent.\n",
    "\n",
    "- **Logistic loss:** \n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{logistic}}(h_\\theta(x), y) = \\log(1+\\exp(-h_\\theta(x) \\cdot y))\n",
    "\\end{equation}\n",
    "For large positive values of $h_\\theta(x) \\cdot y$, $\\exp(-h_\\theta(x) \\cdot y)$ will be very close to zero, so the loss will approach zero; for large negative values $\\log(1+\\exp(-h_\\theta(x) \\cdot y)) \\approx -h_\\theta(x) \\cdot y$ so the loss increases approximately linearly.\n",
    "\n",
    "- **Hinge loss:**\n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{hinge}}(h_\\theta(x), y) = \\max\\{1 - h_\\theta(x) \\cdot y, 0\\}\n",
    "\\end{equation}\n",
    "As long as $h_\\theta(x) \\cdot y \\geq 1$, this loss will be zero, whereas it will increase linearly for negative $h_\\theta(x) \\cdot y \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy = np.linspace(-3,3,1000)\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(hy, np.square(hy-1), c=\"lightgrey\")\n",
    "plt.plot(hy,(hy<=0))\n",
    "plt.plot(hy, np.log(1+np.exp(-hy)))\n",
    "plt.plot(hy, np.maximum(1 - hy, 0))\n",
    "\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-0.05, 5])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"$h_Î¸(x) \\cdot y$\")\n",
    "plt.legend(['Squared', 'Zero-one', 'Logistic', 'Hinge'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Linear Classification Tasks with Support Vector Machines\n",
    "\n",
    "With this framework and set of loss functions laid out, the task of learning a classifier is exactly the same as the regression methods we considered previously.  That is, given a data set we seek to minimize the average loss\n",
    "\\begin{equation}\n",
    "\\min_{\\theta} \\frac{1}{m}\\sum_{i=1}^{m} \\ell(h_{\\theta}(x^{(i)}),y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "Unlike regression, there is virtually no setting (except, of course, just using least-squares loss) where there is an analytical solution to this optimization problem.  Nonetheless, we can still solve it by resorting back to gradient descent (there are alternative optimization approaches for most of these problems, but we'll only cover gradient descent).  To see how this works, we'll next consider a specific machine learning algorithm in detail: support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) result from choosing hinge loss\n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{hinge}}(h_\\theta(x),y) = \\max \\{1-h_\\theta(x)\\cdot y, 0\\}\n",
    "\\end{equation}\n",
    "as the loss function to minimize.  If you have seen support vector machines before, you may have seen them explained in a geometric context, illustrating that they maximize the \"margin\" of separation between examples of different classes.  This is true, and support vector machines absolutely can be viewed this way.  But it is also instructive to simply understand them as minimizing the particular loss function above.\n",
    "\n",
    "Support vector machines also refer either to a linear hypothesis function, or a particular form of nonlinear hypothesis known as a _kernel_ hypothesis (we will get to this in the next workshop); if you ever hear the terms \"linear SVM\" or \"kernel SVM\", this is just the designation of which type of hypothesis function they are using.  That is, linear SVMs use the hypothesis function\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\sum_{j=1}^n \\theta_j x_j = \\theta^T x,\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, let us first add a second feature of the breast cancer dataset: in addition to `area_mean`, we now also consider `concave points_mean` and see how the results differ for **malignant** and **benign** tumors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cells(x='area_mean',y='concave points_mean'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M'][x], cancer_df[cancer_df[\"diagnosis\"]=='M'][y], marker='x', color='C3')\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B'][x], cancer_df[cancer_df[\"diagnosis\"]=='B'][y], marker='+', color='C0')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Mean Concave Points\")\n",
    "    plt.legend(['Malignant','Benign'])\n",
    "    \n",
    "plot_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is obviously some structure to the data here: cells with greater average area and greater numbers of concave points are more likely to be malignant. Remember that we are still trying to find the optimal separating boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cells()\n",
    "plt.plot([0, 1100], [0.08,0], 'k-') # we manually fit a line here by defining the intercepts with the x- and y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as our linear regression line did not pass exactly through all the data points, here, we can't achieve a line that exactly separates all the data points. But there is clearly a structure in which points below the line are \"more likely\" to be benign, and points above the line are \"more likely\" to be malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Intuition: Maximizing the Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again gain a basic understanding of what using a Hinge loss function means graphically. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest points (which are called the support vectors). Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.figure(figsize=(8,6))\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Support Vector Machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on breast cancer data. We will use sklearn's `LinearSVC` class. Alternatively, you could also use the `SVC` class.\n",
    "\n",
    "The scikit-learn library, which we previously discussed in the context of regression, also has built-in algorithms for classification. Using these again just requires the initialization, `.fit()`, and `.predict()` methods, with the additional note that if you want the actual output of the hypothesis function, you need to use the `.decision_function()` call.\n",
    "\n",
    "We also consider here the scikit-learn way of normalizing features.  This is done via the `sklearn.preprocessing.StandardScaler` class. It has a similar interface as the machine learning classes, with a similar method `.fit()` (which computes the mean and variance of a column), and `.transform()` (which converts the data to have zero mean and unit variance). Let's see how this class works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again normlize our features using the StandarScaler class in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "norm = StandardScaler()\n",
    "X_norm = norm.fit_transform(X)  # simultaneously fit and transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall**: StandardScaler normalizes to 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean and standard deviation of Non-normalized X: \", np.mean(X, axis=0), np.std(X, axis=0))\n",
    "print(\"Mean and standard deviation of normalized X: \", np.mean(X_norm, axis=0),np.std(X_norm, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_SVM = LinearSVC(loss='hinge')\n",
    "model_SVM.fit(X_norm, Y)  # Note: There is no need to encode Y. The algorithm does that automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_sklearn(model, norm):\n",
    "    x_range = [0,2600]\n",
    "    y_range = [0, 0.21]\n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=200\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),  # create coordinate mesh\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "    \n",
    "    X_ = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "    zz = model.decision_function(norm.transform(X_)).reshape(grid,grid)\n",
    "    cs = plt.contourf(xx,yy,zz, zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C0')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C3')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Mean Concave Points\")\n",
    "    plt.legend([s1,s2],['Malignant', 'Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_sklearn(model_SVM, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does SVM Classification perform? Let us look at accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_SVM.predict(X_norm) == Y).mean()  # Note: mean() operation on booleans returns the proportion of True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides a shorthand metric function for this: `accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y, model_SVM.predict(X_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just before, we calculated the \"accuracy\" of our SVM to get an understanding of how well our model works. Let us formalize how to properly evaluate classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices plot true negatives (TN) and true positives (TP) vs. false negatives (FN) and false positives (FP). What results is a 2x2 matrix that provides an overview of classifier performance. See the [Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how this looks like for our simplest case, the linear SVM prediction (`model_SVM`) with two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_sklearn(model_SVM, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix for our SVM model\n",
    "import sklearn.metrics\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y, model_SVM.predict(X_norm),labels=[\"M\",\"B\"])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows of the confusion matrix indicate **actual** values, while colums indicate **predicted** values. The `labels` argument allows us to order the confusion matrix. Setting it to `labels = [\"M\",\"B\"]` ensures that the first row and first colum represent malignant entries. Therefore, we get:\n",
    "\n",
    "|  | Malignant | Benign |\n",
    "| --- | --- | --- |\n",
    "| Malignant | TP | FN |\n",
    "| Benign | FP | TN |\n",
    "\n",
    "From this confusion matrix various additional performance metrics can be derived. Let us consider some of the most common ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is the ratio of all correctly classified samples vs. the total number of samples. It is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "The scikit learn implementation is straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(Y, model_SVM.predict(X_norm))\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relating this back to the confusion matrix from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(185+337)/sum(sum(confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision is the ratio of all correctly classified positive samples vs. the total amount of classified positive samples. It provides the proportion of positive identifications that were actually correct and is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation}\n",
    "\n",
    "The scikit learn implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score = precision_score(Y, model_SVM.predict(X_norm), pos_label=\"M\")\n",
    "print(precision_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relating this back to the confusion matrix from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(185)/(185+20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate (Recall)\n",
    "\n",
    "Recall is the ratio of true positives vs. all actual positives. It is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "The scikit learn implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score = recall_score(Y, model_SVM.predict(X_norm), pos_label=\"M\")\n",
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relating this back to the confusion matrix from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(185)/(185+27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Precision-Recall Curves\n",
    "Two other common evaluation metrics are [ROC curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and [precision-recall curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). You can read up about them in the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(model_SVM, X_norm, Y, plot_chance_level=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_estimator(model_SVM, X_norm, Y, plot_chance_level=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Remember that to properly evaluate a prediction model, we need to perform cross-validation (just like we did in a regression setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.6666717529297px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
